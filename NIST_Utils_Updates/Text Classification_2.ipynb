{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from NLPutils import NLPutils as NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer # load the stemmer module from NLTK\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import time\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import py_trees\n",
    "#import behaviours as be\n",
    "from py_trees.blackboard import Blackboard\n",
    "from scipy import spatial\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAAdata(object):\n",
    "    def __init__(self,text,vital,inter):\n",
    "        self.text = text\n",
    "        self.vital = vital\n",
    "        self.inter = inter\n",
    "        \n",
    "def load_RAA_data(path, cv = True): \n",
    "    df = pd.read_excel(path)\n",
    "    interset = set()\n",
    "    interdict = dict()\n",
    "    narratives = df['Narrative']\n",
    "    narratives = [i for i in narratives]\n",
    "    inters = df['Interventions']\n",
    "    vitals = df['Vitals']\n",
    "    vitals = [i for i in vitals]\n",
    "    interventions = []\n",
    "    for item in inters:\n",
    "        inter = item.strip('{}').split('}{')\n",
    "        inter = [i.split(':')[-1].strip().lower() for i in inter]\n",
    "        c_int = []\n",
    "        for j in inter:\n",
    "            interset.add(j)\n",
    "            if j in interdict:\n",
    "                interdict[j] += 1\n",
    "            else:\n",
    "                interdict[j] = 1\n",
    "            c_int.append(j)\n",
    "        interventions.append(c_int)\n",
    "    for inter in list(interdict):\n",
    "        if cv and interdict[inter] < 20: del interdict[inter]\n",
    "    data = [RAAdata(item,vitals[idx],interventions[idx]) for idx,item in enumerate(narratives)]\n",
    "    \n",
    "    return data,interdict\n",
    "\n",
    "def fullmatch(regex, string, flags=0):\n",
    "    \"\"\"Emulate python-3.4 re.fullmatch().\"\"\"\n",
    "    return re.match(\"(?:\" + regex + r\")\\Z\", string, flags=flags)\n",
    "\n",
    "# preprocess utils\n",
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence):\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "def weighted_precision_recall_f1_util (y_test, y_pre, weight = None):\n",
    "    tp, fp, fn = [0. for _ in range(len(y_pre[0]))], [0. for _ in range(len(y_pre[0]))], \\\n",
    "    [0. for _ in range(len(y_pre[0]))]\n",
    "    for idx in range(len(y_pre)):\n",
    "        for i in range(len(y_pre[idx])):\n",
    "            if y_pre[idx][i] == 1 and y_test[idx][i] == 1: tp[i] += 1\n",
    "            elif y_pre[idx][i] == 1 and y_test[idx][i] == 0: fp[i] += 1\n",
    "            elif y_pre[idx][i] == 0 and y_test[idx][i] == 1: fn[i] += 1\n",
    "    precision = [tp[i] / (tp[i] + fp[i]) if tp[i] > 0 or fp[i] > 0 else 0. for i in range(len(tp))]\n",
    "    recall = [tp[i] / (tp[i] + fn[i]) if tp[i] > 0 or fn[i] > 0 else 0. for i in range(len(tp))]\n",
    "    f1 = [2 * precision[i] * recall[i] / (precision[i] + recall[i]) \\\n",
    "         if precision[i] > 0 or recall[i] > 0 else 0. for i in range(len(tp))]\n",
    "    return np.average(precision, weights = weight), np.average(recall, weights = weight), \\\n",
    "np.average(f1, weights = weight)\n",
    "\n",
    "def weighted_precision (y_test, y_pre, weight = None):\n",
    "    precision, _, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return precision\n",
    "\n",
    "def weighted_recall (y_test, y_pre, weight = None):\n",
    "    _, recall, _ = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return recall\n",
    "\n",
    "def weighted_f1 (y_test, y_pre, weight = None):\n",
    "    _, _, f1 = weighted_precision_recall_f1_util (y_test, y_pre, weight)\n",
    "    return f1\n",
    "\n",
    "def show_results(scores):\n",
    "    metrics = ['test_precision_weighted','test_recall_weighted', 'test_f1_weighted',\\\n",
    "            'test_precision_micro', 'test_recall_micro', 'test_f1_micro']\n",
    "    for metric in metrics:\n",
    "        print metric + ':' + '%.2f' % np.average(scores[metric])\n",
    "        \n",
    "def risk_factor(gt, probs, preds):\n",
    "    risk = []\n",
    "    for idx,case in enumerate(probs):\n",
    "        r = 0\n",
    "        for i,prob in enumerate(case):\n",
    "            if preds[idx][i] == 1 and gt[idx][i] == 0:\n",
    "                r += prob * int2fp_score[num2int[i]] / sum(gt[idx])\n",
    "            if preds[idx][i] == 0 and gt[idx][i] == 1:\n",
    "                r += prob * int2fn_score[num2int[i]] / sum(gt[idx])\n",
    "        risk.append(r)\n",
    "    return sum(risk) / len(risk)\n",
    "\n",
    "def trans_prob(probs):\n",
    "    transed_prob = [[0.] * len(probs) for _ in range(len(probs[0]))]\n",
    "    for idx, res in enumerate(probs):\n",
    "        for i, p in enumerate(res):\n",
    "            if len(p) < 2: transed_prob[i][idx] = 1. - p[0]\n",
    "            else: transed_prob[i][idx] = p[1]\n",
    "                \n",
    "    return transed_prob\n",
    "\n",
    "def show_test_results(gt, res, prob, class_weight):\n",
    "    print \"precision_micro\" + ':' + '%.2f' % precision_score(gt, res, average = 'micro')\n",
    "    print \"recall_micro\" + ':' + '%.2f' % recall_score(gt, res, average = 'micro')\n",
    "    print \"f1_micro\" + ':' + '%.2f' % f1_score(gt, res, average = 'micro')\n",
    "    print \"precision_weighted\" + ':' + '%.2f' % weighted_precision(gt, res, class_weight)\n",
    "    print \"recall_weighted\" + ':' + '%.2f' % weighted_recall(gt, res, class_weight)\n",
    "    print \"f1_weighted\" + ':' + '%.2f' % weighted_f1(gt, res, class_weight)\n",
    "    print \"risk_factor\" + ':' + '%.4f' % risk_factor(gt, prob, res)\n",
    "    \n",
    "def filtering(res, prob, threshold):\n",
    "    for idx, case in enumerate(res):\n",
    "        for i in range(len(case)):\n",
    "            if prob[idx][i] < threshold:\n",
    "                res[idx][i] = 0\n",
    "                prob[idx][i] = 0.\n",
    "    return res, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labeled cases\n",
    "docu = './RAA_train.xlsx'\n",
    "df = pd.read_excel(docu)\n",
    "train_narratives = df['Narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docu = './RAA_1000_test.xlsx'\n",
    "df = pd.read_excel(docu)\n",
    "test_narratives = df['Narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b832ac164604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_vec_list.txr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_vec' is not defined"
     ]
    }
   ],
   "source": [
    "##SKIP##with open('test_vec_list.txr','w') as fo:\n",
    "    pickle.dump(test_vec, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-10-94f7e295b585>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-94f7e295b585>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    test_vec = pickle.load(fo)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "##SKIP##with open('test_vec_list.txr') as fo:\n",
    "    test_vec = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SKIP##len(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "narra,intdict = load_RAA_data('./RAA_train.xlsx')\n",
    "test_narra, _ = load_RAA_data('./RAA_1000_test.xlsx', cv = False)\n",
    "risk_route = './Intervention Safety Sheet.xlsx'\n",
    "df_risk = pd.read_excel(risk_route)\n",
    "int2fn_score = dict()\n",
    "int2fp_score = dict()\n",
    "for row in df_risk.iterrows():\n",
    "    name = row[1]['Intervention'].split('\\'')[1]\n",
    "    FN_score, FP_score = 0, 0\n",
    "    if not pd.isnull(row[1]['If NOT Done When Indicated']):\n",
    "        FN_score = int(row[1]['If NOT Done When Indicated'])\n",
    "    if not pd.isnull(row[1]['If Done When NOT Indicated']):\n",
    "        FP_score = int(row[1]['If Done When NOT Indicated'])\n",
    "    if not FN_score or not FP_score or (name not in intdict):\n",
    "        continue\n",
    "    int2fn_score[name] = FN_score\n",
    "    int2fp_score[name] = FP_score\n",
    "int2num = dict()\n",
    "num2int = dict()\n",
    "for i,key in enumerate(int2fn_score):\n",
    "    int2num[key] = i\n",
    "    num2int[i] = key\n",
    "#n = NLP()\n",
    "# load technical n-grams\n",
    "fo = open('ngrams.txt')\n",
    "ngrams = set()\n",
    "for line in fo:\n",
    "    if line == '\\n': continue\n",
    "    ngrams.add(line.strip('\\n'))\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_safety = [dict() for _ in range(len(int2num))]\n",
    "for idx in range(len(num2int)):\n",
    "    inter_safety[idx][0] = 1. / int2fn_score[num2int[idx]]\n",
    "    inter_safety[idx][1] = 1. / int2fp_score[num2int[idx]]\n",
    "inter_safety_dic = dict()\n",
    "for idx in range(len(num2int)):\n",
    "    inter_safety_dic[idx] = int2fp_score[num2int[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = [i.text for i in narra]\n",
    "total_inter = [i.inter for i in narra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,_ = train_test_split(narra, random_state=46, test_size=.2, shuffle=True)\n",
    "train_text = [i.text for i in train]\n",
    "train_inter = [i.inter for i in train]\n",
    "test_text = [i.text for i in test_narra]\n",
    "test_inter = [i.inter for i in test_narra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tay/anaconda3/envs/py2_NIST/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [u'abov', u'afterward', u'alon', u'alreadi', u'alway', u'ani', u'anoth', u'anyon', u'anyth', u'anywher', u'becam', u'becaus', u'becom', u'befor', u'besid', u'cri', u'describ', u'dure', u'els', u'elsewher', u'empti', u'everi', u'everyon', u'everyth', u'everywher', u'fifti', u'forti', u'henc', u'hereaft', u'herebi', u'howev', u'hundr', u'inde', u'mani', u'meanwhil', u'moreov', u'nobodi', u'noon', u'noth', u'nowher', u'onc', u'onli', u'otherwis', u'ourselv', u'perhap', u'pleas', u'sever', u'sinc', u'sincer', u'sixti', u'someon', u'someth', u'sometim', u'somewher', u'themselv', u'thenc', u'thereaft', u'therebi', u'therefor', u'togeth', u'twelv', u'twenti', u'veri', u'whatev', u'whenc', u'whenev', u'wherea', u'whereaft', u'wherebi', u'wherev', u'whi', u'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = cleanPunc(text)\n",
    "    text = keepAlpha(text)\n",
    "    text = stemming(text)\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = [[int2num[inter] for inter in case if inter in int2num] for case in train_inter]\n",
    "encoded_y_train = np.array([[int(num in case) for num in range(len(int2num))] for case in y_train])\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = [[int2num[inter] for inter in case if inter in int2num] for case in test_inter]\n",
    "encoded_y_test = np.array([[int(num in case) for num in range(len(int2num))] for case in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(total_text)\n",
    "x_total = vectorizer.transform(total_text)\n",
    "y_total = [[int2num[inter] for inter in case if inter in int2num] for case in total_inter]\n",
    "encoded_y_total = np.array([[int(num in case) for num in range(len(int2num))] for case in y_total])\n",
    "class_weight = np.sum(encoded_y_total, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),vocabulary = ngrams,\\\n",
    "                             preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "x_train_ngram = vectorizer.transform(train_text)\n",
    "y_train = [[int2num[inter] for inter in case if inter in int2num] for case in train_inter]\n",
    "encoded_y_train = np.array([[int(num in case) for num in range(len(int2num))] for case in y_train])\n",
    "x_test_ngram = vectorizer.transform(test_text)\n",
    "y_test = [[int2num[inter] for inter in case if inter in int2num] for case in test_inter]\n",
    "encoded_y_test = np.array([[int(num in case) for num in range(len(int2num))] for case in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,4),vocabulary = ngrams,\\\n",
    "                             preprocessor = preprocess, stop_words = 'english', norm='l2')\n",
    "vectorizer.fit(total_text)\n",
    "x_total_ngram = vectorizer.transform(total_text)\n",
    "y_total = [[int2num[inter] for inter in case if inter in int2num] for case in total_inter]\n",
    "encoded_y_total = np.array([[int(num in case) for num in range(len(int2num))] for case in y_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'precision_weighted': make_scorer(weighted_precision, weight = class_weight),\n",
    "           'recall_weighted': make_scorer(weighted_recall, weight = class_weight),\n",
    "           'f1_weighted': make_scorer(weighted_f1, weight = class_weight),\n",
    "           'precision_micro': 'precision_micro',\n",
    "           'recall_micro': 'recall_micro',\n",
    "           'f1_micro': 'f1_micro'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-391047fbba69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m scores_1 = cross_validate(clf, x_total, encoded_y_total, scoring=scoring,\n\u001b[0m\u001b[1;32m      3\u001b[0m                          cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_total' is not defined"
     ]
    }
   ],
   "source": [
    "##SKIP for now##\n",
    "###look into incremental training#\n",
    "#Look into sklearn option to spread work across processors#\n",
    "clf = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_1 = cross_validate(clf, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.9412081242\n",
      "precision_micro:0.92\n",
      "recall_micro:0.88\n",
      "f1_micro:0.90\n",
      "precision_weighted:0.88\n",
      "recall_weighted:0.86\n",
      "f1_weighted:0.87\n",
      "risk_factor:0.2360\n"
     ]
    }
   ],
   "source": [
    "##SKIP for now##\n",
    "###look into incremental training#\n",
    "#Look into sklearn option to spread work across processors#\n",
    "clf = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_1 = clf.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_1 = clf.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_1, y_pos_1, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "scores_2 = cross_validate(clf_rf, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.86\n",
      "test_recall_weighted:0.70\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.89\n",
      "test_recall_micro:0.69\n",
      "test_f1_micro:0.78\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.104833841324\n",
      "precision_micro:0.90\n",
      "recall_micro:0.71\n",
      "f1_micro:0.79\n",
      "precision_weighted:0.79\n",
      "recall_weighted:0.68\n",
      "f1_weighted:0.70\n",
      "risk_factor:0.4160\n"
     ]
    }
   ],
   "source": [
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_2 = clf_rf.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_2 = clf_rf.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_2, trans_prob(y_pos_2), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "scores_3 = cross_validate(clf_dt, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.81\n",
      "test_f1_weighted:0.80\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.81\n",
      "test_f1_micro:0.81\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0083110332489\n",
      "precision_micro:0.84\n",
      "recall_micro:0.82\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.81\n",
      "recall_weighted:0.80\n",
      "f1_weighted:0.80\n",
      "risk_factor:0.2785\n"
     ]
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier()\n",
    "clf_dt.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_3 = clf_dt.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_3 = clf_dt.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_3, trans_prob(y_pos_3), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------->\n",
      "('Distance: n_neigh', 5)\n",
      "test_precision_weighted:0.78\n",
      "test_recall_weighted:0.77\n",
      "test_f1_weighted:0.75\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.76\n",
      "test_f1_micro:0.79\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 6)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.77\n",
      "test_f1_weighted:0.76\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.77\n",
      "test_f1_micro:0.79\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 7)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.76\n",
      "test_f1_weighted:0.75\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.75\n",
      "test_f1_micro:0.78\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 8)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.77\n",
      "test_f1_weighted:0.75\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.75\n",
      "test_f1_micro:0.79\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 9)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.74\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.74\n",
      "test_f1_micro:0.78\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 10)\n",
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.76\n",
      "test_f1_weighted:0.75\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.74\n",
      "test_f1_micro:0.78\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 11)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.73\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.73\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 12)\n",
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.74\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.73\n",
      "test_f1_micro:0.78\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 13)\n",
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.73\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 14)\n",
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.73\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.73\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Distance: n_neigh', 15)\n",
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.77\n"
     ]
    }
   ],
   "source": [
    "##TAP##\n",
    "##K Nearest Neighbor##\n",
    "#Distance#\n",
    "from sklearn import neighbors\n",
    "n_neighbors = 15\n",
    "weights = 'distance'\n",
    "\n",
    "for i_n_neighbors in range(5,n_neighbors+1):\n",
    "    clf_knn = neighbors.KNeighborsClassifier(i_n_neighbors, weights=weights)\n",
    "    scores_3 = cross_validate(clf_knn, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "    print('------------------------------------->')\n",
    "    print('Distance: n_neigh',i_n_neighbors)\n",
    "    show_results(scores_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.08958005905\n",
      "precision_micro:0.86\n",
      "recall_micro:0.74\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.80\n",
      "recall_weighted:0.73\n",
      "f1_weighted:0.74\n",
      "risk_factor:0.3889\n"
     ]
    }
   ],
   "source": [
    "clf_knn = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf_knn.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_3 = clf_knn.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_3 = clf_knn.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_3, trans_prob(y_pos_3), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------->\n",
      "('Uniform: n_neigh', 5)\n",
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.74\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.75\n",
      "test_f1_micro:0.78\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 6)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.71\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.85\n",
      "test_recall_micro:0.70\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 7)\n",
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.73\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.74\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 8)\n",
      "test_precision_weighted:0.79\n",
      "test_recall_weighted:0.71\n",
      "test_f1_weighted:0.71\n",
      "test_precision_micro:0.84\n",
      "test_recall_micro:0.70\n",
      "test_f1_micro:0.76\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 9)\n",
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.73\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.77\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 10)\n",
      "test_precision_weighted:0.78\n",
      "test_recall_weighted:0.70\n",
      "test_f1_weighted:0.70\n",
      "test_precision_micro:0.84\n",
      "test_recall_micro:0.69\n",
      "test_f1_micro:0.76\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 11)\n",
      "test_precision_weighted:0.76\n",
      "test_recall_weighted:0.72\n",
      "test_f1_weighted:0.71\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.71\n",
      "test_f1_micro:0.76\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 12)\n",
      "test_precision_weighted:0.78\n",
      "test_recall_weighted:0.70\n",
      "test_f1_weighted:0.70\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.69\n",
      "test_f1_micro:0.75\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 13)\n",
      "test_precision_weighted:0.76\n",
      "test_recall_weighted:0.72\n",
      "test_f1_weighted:0.70\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.70\n",
      "test_f1_micro:0.75\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 14)\n",
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.69\n",
      "test_f1_weighted:0.69\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.68\n",
      "test_f1_micro:0.75\n",
      "------------------------------------->\n",
      "('Uniform: n_neigh', 15)\n",
      "test_precision_weighted:0.76\n",
      "test_recall_weighted:0.71\n",
      "test_f1_weighted:0.70\n",
      "test_precision_micro:0.81\n",
      "test_recall_micro:0.70\n",
      "test_f1_micro:0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "n_neighbors = 15\n",
    "weights = 'uniform'\n",
    "\n",
    "for i_n_neighbors in range(5,n_neighbors+1):\n",
    "    clf_knn = neighbors.KNeighborsClassifier(i_n_neighbors, weights=weights)\n",
    "    scores_3 = cross_validate(clf_knn, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "    print('------------------------------------->')\n",
    "    print('Uniform: n_neigh',i_n_neighbors)\n",
    "    show_results(scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.80\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.72\n",
      "test_precision_micro:0.82\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.77\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68601703644\n",
      "precision_micro:0.86\n",
      "recall_micro:0.74\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.80\n",
      "recall_weighted:0.73\n",
      "f1_weighted:0.74\n",
      "risk_factor:0.3871\n"
     ]
    }
   ],
   "source": [
    "clf_knn = neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
    "clf_knn.fit(x_train, encoded_y_train)\n",
    "start_time = time.time()\n",
    "y_pre_3 = clf_knn.predict(x_test)\n",
    "elapsed_time = time.time() - start_time\n",
    "print elapsed_time\n",
    "y_pos_3 = clf_knn.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_3, trans_prob(y_pos_3), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benadryl en route\n",
      "0.06767252225826034\n",
      "golf ball\n",
      "0.06209289895747574\n",
      "address store\n",
      "0.05219661975393536\n",
      "front driver side corner\n",
      "0.042514874406786744\n",
      "chin lift\n",
      "0.034479024135152206\n",
      "patient lying unresponsive\n",
      "0.0312960987037582\n",
      "breath away\n",
      "0.029438043227958434\n",
      "drank cold water\n",
      "0.02500171368974516\n",
      "look well\n",
      "0.017344479974113754\n",
      "ankle pain\n",
      "0.016592926589528927\n",
      "hospitalized multiple\n",
      "0.014676397430999638\n",
      "child care issue\n",
      "0.014098247445898708\n",
      "oxygen use\n",
      "0.013201628921830426\n",
      "past feel\n",
      "0.010114745036779107\n",
      "equal mobility\n",
      "0.008374562373298294\n",
      "house yesterday\n",
      "0.007889074885254187\n",
      "ambulatory onscene\n",
      "0.007828201796429762\n",
      "hallway laying\n",
      "0.0074822465419892\n",
      "medication administraton\n",
      "0.007366061866284456\n",
      "free other\n",
      "0.007141535147213692\n",
      "current medicaiton\n",
      "0.006911205931570831\n",
      "insulin pump\n",
      "0.00658760343591258\n",
      "confused behavior\n",
      "0.005364718922628631\n",
      "initial fall\n",
      "0.005299895046551835\n",
      "inhaler use\n",
      "0.0051942454992976615\n",
      "black female lying supine\n",
      "0.005153685338033144\n",
      "anterior part\n",
      "0.00464826572332659\n",
      "intubated numerous\n",
      "0.004584346984873422\n",
      "little nausea\n",
      "0.00438134416890938\n",
      "copious amount\n",
      "0.004358964919443752\n"
     ]
    }
   ],
   "source": [
    "heap = []\n",
    "for idx, importance in enumerate(clf_dt.feature_importances_):\n",
    "    heap.append((importance, idx))\n",
    "heap.sort(reverse = True)\n",
    "for item in heap[:30]:\n",
    "    print vectorizer.get_feature_names()[item[1]]\n",
    "    print item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_4 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_4 = cross_validate(clf_4, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.82\n",
      "test_recall_weighted:0.75\n",
      "test_f1_weighted:0.76\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.75\n",
      "test_f1_micro:0.81\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.89\n",
      "recall_micro:0.77\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.77\n",
      "f1_weighted:0.78\n",
      "risk_factor:0.3177\n"
     ]
    }
   ],
   "source": [
    "clf_4 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_4.fit(x_train_ngram, encoded_y_train)\n",
    "y_pre_4 = clf_4.predict(x_test_ngram)\n",
    "y_pos_4 = clf_4.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_4, y_pos_4, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_1 = RandomForestClassifier()\n",
    "scores_5 = cross_validate(clf_rf_1, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.83\n",
      "test_recall_weighted:0.66\n",
      "test_f1_weighted:0.69\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.66\n",
      "test_f1_micro:0.75\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.88\n",
      "recall_micro:0.66\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.82\n",
      "recall_weighted:0.67\n",
      "f1_weighted:0.69\n",
      "risk_factor:0.4570\n"
     ]
    }
   ],
   "source": [
    "clf_rf_1 = RandomForestClassifier()\n",
    "clf_rf_1.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_5 = clf_rf_1.predict(x_test_ngram)\n",
    "y_pos_5 = clf_rf_1.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_5, trans_prob(y_pos_5), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.03353867214\n"
     ]
    }
   ],
   "source": [
    "y_pos_5_c = [[0.] * len(y_pos_5) for _ in range(len(y_pos_5[0]))]\n",
    "for idx, res in enumerate(y_pos_5):\n",
    "    for i, p in enumerate(res):\n",
    "        if len(p) < 2: y_pos_5_c[i][idx] = 1. - p[0]\n",
    "        else: y_pos_5_c[i][idx] = p[1]\n",
    "\n",
    "risk = []\n",
    "for case in y_pos_5_c:\n",
    "    r = 0\n",
    "    for i,pos in enumerate(case):\n",
    "        r += pos * num2risk[num2int[i]]\n",
    "    risk.append(r)\n",
    "print sum(risk)/len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_1 = DecisionTreeClassifier()\n",
    "scores_6 = cross_validate(clf_dt_1, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.73\n",
      "test_recall_weighted:0.74\n",
      "test_f1_weighted:0.73\n",
      "test_precision_micro:0.75\n",
      "test_recall_micro:0.74\n",
      "test_f1_micro:0.75\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.77\n",
      "recall_micro:0.75\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.73\n",
      "recall_weighted:0.72\n",
      "f1_weighted:0.72\n",
      "risk_factor:0.4762\n"
     ]
    }
   ],
   "source": [
    "clf_dt_1 = DecisionTreeClassifier()\n",
    "clf_dt_1.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_6 = clf_dt_1.predict(x_test_ngram)\n",
    "y_pos_6 = clf_dt_1.predict_proba(x_test_ngram)\n",
    "\n",
    "show_test_results(encoded_y_test, y_pre_6, trans_prob(y_pos_6), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.92539356605\n"
     ]
    }
   ],
   "source": [
    "y_pos_6_c = [[0.] * len(y_pos_6) for _ in range(len(y_pos_6[0]))]\n",
    "for idx, res in enumerate(y_pos_6):\n",
    "    for i, p in enumerate(res):\n",
    "        if len(p) < 2: y_pos_6_c[i][idx] = 1. - p[0]\n",
    "        else: y_pos_6_c[i][idx] = p[1]\n",
    "\n",
    "risk = []\n",
    "for case in y_pos_6_c:\n",
    "    r = 0\n",
    "    for i,pos in enumerate(case):\n",
    "        r += pos * num2risk[num2int[i]]\n",
    "    risk.append(r)\n",
    "print sum(risk)/len(risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Class label 2 not present.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-7b8461f3988f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter_safety_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# cv_results = cross_validate(clf, x_train, y_train, cv=5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoded_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pre_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pos_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 215\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/Users/sileshu/anaconda/lib/python2.7/site-packages/sklearn/utils/class_weight.pyc\u001b[0m in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Class label {} not present.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Class label 2 not present."
     ]
    }
   ],
   "source": [
    "clf_7 = OneVsRestClassifier(SVC(probability=True, class_weight = inter_safety_dic))\n",
    "# cv_results = cross_validate(clf, x_train, y_train, cv=5)\n",
    "clf_7.fit(x_train,encoded_y_train)\n",
    "y_pre_7 = clf_7.predict(x_test)\n",
    "y_pos_7 = clf_7.predict_proba(x_test)\n",
    "\n",
    "print precision_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print precision_score(encoded_y_test,y_pre_7,average = 'micro')\n",
    "print recall_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print recall_score(encoded_y_test,y_pre_7,average = 'micro')\n",
    "print f1_score(encoded_y_test,y_pre_7,average = 'weighted')\n",
    "print f1_score(encoded_y_test,y_pre_7,average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_2 = RandomForestClassifier(class_weight = inter_safety)\n",
    "scores_7 = cross_validate(clf_rf_2, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.84\n",
      "test_recall_weighted:0.65\n",
      "test_f1_weighted:0.67\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.64\n",
      "test_f1_micro:0.74\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.87\n",
      "recall_micro:0.64\n",
      "f1_micro:0.74\n",
      "precision_weighted:0.79\n",
      "recall_weighted:0.65\n",
      "f1_weighted:0.66\n",
      "risk_factor:0.4941\n"
     ]
    }
   ],
   "source": [
    "clf_rf_2 = RandomForestClassifier(class_weight = inter_safety)\n",
    "clf_rf_2.fit(x_train,encoded_y_train)\n",
    "y_pre_8 = clf_rf_2.predict(x_test)\n",
    "y_pos_8 = clf_rf_2.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_8, trans_prob(y_pos_8), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_2 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "scores_8 = cross_validate(clf_dt_2, x_total, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.77\n",
      "test_recall_weighted:0.78\n",
      "test_f1_weighted:0.77\n",
      "test_precision_micro:0.79\n",
      "test_recall_micro:0.78\n",
      "test_f1_micro:0.78\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.80\n",
      "recall_micro:0.79\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.77\n",
      "recall_weighted:0.78\n",
      "f1_weighted:0.77\n",
      "risk_factor:0.3803\n"
     ]
    }
   ],
   "source": [
    "clf_dt_2 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "clf_dt_2.fit(x_train,encoded_y_train)\n",
    "y_pre_9 = clf_dt_2.predict(x_test)\n",
    "y_pos_9 = clf_dt_2.predict_proba(x_test)\n",
    "show_test_results(encoded_y_test, y_pre_9, trans_prob(y_pos_9), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_rf_3 = RandomForestClassifier(class_weight = inter_safety)\n",
    "scores_9 = cross_validate(clf_rf_3, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.82\n",
      "test_recall_weighted:0.62\n",
      "test_f1_weighted:0.64\n",
      "test_precision_micro:0.88\n",
      "test_recall_micro:0.61\n",
      "test_f1_micro:0.72\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lidolla/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.87\n",
      "recall_micro:0.61\n",
      "f1_micro:0.72\n",
      "precision_weighted:0.73\n",
      "recall_weighted:0.61\n",
      "f1_weighted:0.62\n",
      "risk_factor:0.4827\n"
     ]
    }
   ],
   "source": [
    "clf_rf_3 = RandomForestClassifier(class_weight = inter_safety)\n",
    "clf_rf_3.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_10 = clf_rf_3.predict(x_test_ngram)\n",
    "y_pos_10 = clf_rf_3.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_10, trans_prob(y_pos_10), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dt_3 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "scores_10 = cross_validate(clf_dt_3, x_total_ngram, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.71\n",
      "test_recall_weighted:0.73\n",
      "test_f1_weighted:0.71\n",
      "test_precision_micro:0.74\n",
      "test_recall_micro:0.72\n",
      "test_f1_micro:0.73\n"
     ]
    }
   ],
   "source": [
    "show_results(scores_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.75\n",
      "recall_micro:0.73\n",
      "f1_micro:0.74\n",
      "precision_weighted:0.71\n",
      "recall_weighted:0.70\n",
      "f1_weighted:0.70\n",
      "risk_factor:0.5455\n"
     ]
    }
   ],
   "source": [
    "clf_dt_3 = DecisionTreeClassifier(class_weight = inter_safety)\n",
    "clf_dt_3.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_11 = clf_dt_3.predict(x_test_ngram)\n",
    "y_pos_11 = clf_dt_3.predict_proba(x_test_ngram)\n",
    "show_test_results(encoded_y_test, y_pre_11, trans_prob(y_pos_11), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.78\n",
      "recall_micro:0.74\n",
      "f1_micro:0.76\n",
      "precision_weighted:0.73\n",
      "recall_weighted:0.72\n",
      "f1_weighted:0.72\n",
      "risk_factor:0.4641\n"
     ]
    }
   ],
   "source": [
    "clf_dt_4 = DecisionTreeClassifier()\n",
    "clf_dt_4.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_12 = clf_dt_4.predict(x_test_ngram)\n",
    "y_pos_12 = clf_dt_4.predict_proba(x_test_ngram)\n",
    "y_pre_12, y_pos_12 = filtering(y_pre_12, trans_prob(y_pos_12), .11)\n",
    "show_test_results(encoded_y_test, y_pre_12, y_pos_12, class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.84\n",
      "recall_micro:0.81\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.81\n",
      "recall_weighted:0.80\n",
      "f1_weighted:0.80\n",
      "risk_factor:0.2903\n"
     ]
    }
   ],
   "source": [
    "clf_dt_5 = DecisionTreeClassifier()\n",
    "clf_dt_5.fit(x_train,encoded_y_train)\n",
    "y_pre_13 = clf_dt_5.predict(x_test)\n",
    "y_pos_13 = clf_dt_5.predict_proba(x_test)\n",
    "y_pre_13, y_pos_13 = filtering(y_pre_13, trans_prob(y_pos_13), .11)\n",
    "show_test_results(encoded_y_test, y_pre_13, y_pos_13, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.88\n",
      "recall_micro:0.65\n",
      "f1_micro:0.75\n",
      "precision_weighted:0.78\n",
      "recall_weighted:0.65\n",
      "f1_weighted:0.66\n",
      "risk_factor:0.4262\n"
     ]
    }
   ],
   "source": [
    "clf_rf_4 = RandomForestClassifier()\n",
    "clf_rf_4.fit(x_train_ngram,encoded_y_train)\n",
    "y_pre_14 = clf_rf_4.predict(x_test_ngram)\n",
    "y_pos_14 = clf_rf_4.predict_proba(x_test_ngram)\n",
    "y_pre_14, y_pos_14 = filtering(y_pre_14, trans_prob(y_pos_14), .11)\n",
    "show_test_results(encoded_y_test, y_pre_14, y_pos_14, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.90\n",
      "recall_micro:0.72\n",
      "f1_micro:0.80\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.72\n",
      "f1_weighted:0.74\n",
      "risk_factor:0.3941\n"
     ]
    }
   ],
   "source": [
    "clf_rf_5 = RandomForestClassifier()\n",
    "clf_rf_5.fit(x_train,encoded_y_train)\n",
    "y_pre_15 = clf_rf_5.predict(x_test)\n",
    "y_pos_15 = clf_rf_5.predict_proba(x_test)\n",
    "y_pre_15, y_pos_15 = filtering(y_pre_15, trans_prob(y_pos_15), .11)\n",
    "show_test_results(encoded_y_test, y_pre_15, y_pos_15, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.64\n",
      "test_recall_weighted:0.64\n",
      "test_f1_weighted:0.62\n",
      "test_precision_micro:0.83\n",
      "test_recall_micro:0.65\n",
      "test_f1_micro:0.73\n"
     ]
    }
   ],
   "source": [
    "# clfs using feature vectors\n",
    "clf_16 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "scores_16 = cross_validate(clf_16, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.81\n",
      "recall_micro:0.64\n",
      "f1_micro:0.72\n",
      "precision_weighted:0.62\n",
      "recall_weighted:0.63\n",
      "f1_weighted:0.61\n",
      "risk_factor:0.4308\n"
     ]
    }
   ],
   "source": [
    "clf_16 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_16.fit(train_vec, encoded_y_total)\n",
    "y_pre_16 = clf_16.predict(test_vec)\n",
    "y_pos_16 = clf_16.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_16, y_pos_16, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_precision_weighted:0.72\n",
      "test_recall_weighted:0.66\n",
      "test_f1_weighted:0.67\n",
      "test_precision_micro:0.80\n",
      "test_recall_micro:0.66\n",
      "test_f1_micro:0.72\n"
     ]
    }
   ],
   "source": [
    "clf_rf_6 = RandomForestClassifier()\n",
    "scores_17 = cross_validate(clf_rf_6, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.76\n",
      "recall_micro:0.60\n",
      "f1_micro:0.67\n",
      "precision_weighted:0.65\n",
      "recall_weighted:0.60\n",
      "f1_weighted:0.60\n",
      "risk_factor:0.5956\n"
     ]
    }
   ],
   "source": [
    "clf_rf_6 = RandomForestClassifier()\n",
    "clf_rf_6.fit(train_vec, encoded_y_total)\n",
    "y_pre_17 = clf_rf_6.predict(test_vec)\n",
    "y_pos_17 = clf_rf_6.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_17, trans_prob(y_pos_17), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0d12d539f7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_dt_6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m scores_18 = cross_validate(clf_dt_6, train_vec, encoded_y_total, scoring=scoring,\n\u001b[0m\u001b[1;32m      3\u001b[0m                          cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vec' is not defined"
     ]
    }
   ],
   "source": [
    "clf_dt_6 = DecisionTreeClassifier()\n",
    "scores_18 = cross_validate(clf_dt_6, train_vec, encoded_y_total, scoring=scoring,\n",
    "                         cv=5, n_jobs=-1, return_train_score=False, return_estimator=True)\n",
    "show_results(scores_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2455c2fbe6d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_dt_6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf_dt_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_y_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pre_18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_dt_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pos_18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_dt_6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshow_test_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pre_18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pos_18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_vec' is not defined"
     ]
    }
   ],
   "source": [
    "clf_dt_6 = DecisionTreeClassifier()\n",
    "clf_dt_6.fit(train_vec, encoded_y_total)\n",
    "y_pre_18 = clf_dt_6.predict(test_vec)\n",
    "y_pos_18 = clf_dt_6.predict_proba(test_vec)\n",
    "show_test_results(encoded_y_test, y_pre_18, trans_prob(y_pos_18), class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.92\n",
      "recall_micro:0.88\n",
      "f1_micro:0.90\n",
      "precision_weighted:0.88\n",
      "recall_weighted:0.86\n",
      "f1_weighted:0.87\n",
      "risk_factor:0.2294\n"
     ]
    }
   ],
   "source": [
    "clf_19 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_19.fit(x_train, encoded_y_train)\n",
    "y_pre_19 = clf_19.predict(x_test)\n",
    "y_pos_19 = clf_19.predict_proba(x_test)\n",
    "y_pre_19, y_pos_19 = filtering(y_pre_19, y_pos_19, .11)\n",
    "show_test_results(encoded_y_test, y_pre_19, y_pos_19, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_micro:0.89\n",
      "recall_micro:0.77\n",
      "f1_micro:0.83\n",
      "precision_weighted:0.83\n",
      "recall_weighted:0.77\n",
      "f1_weighted:0.78\n",
      "risk_factor:0.3084\n"
     ]
    }
   ],
   "source": [
    "clf_20 = OneVsRestClassifier(SVC(kernel = 'linear', probability=True))\n",
    "clf_20.fit(x_train_ngram, encoded_y_train)\n",
    "y_pre_20 = clf_20.predict(x_test_ngram)\n",
    "y_pos_20 = clf_20.predict_proba(x_test_ngram)\n",
    "y_pre_20, y_pos_20 = filtering(y_pre_20, y_pos_20, .11)\n",
    "show_test_results(encoded_y_test, y_pre_20, y_pos_20, class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function preprocess at 0x1a509c9b18>,\n",
       "        smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "        sublinear_tf=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_vectorizer.txt', 'w') as uv:\n",
    "    pickle.dump(vectorizer, uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d227e3c706d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'linear_svmclf_unigram.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "with open('linear_svmclf_unigram.txt', 'w') as ls:\n",
    "    pickle.dump(clf, ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-38dd94c8b46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'linear_svmclf_feat_vec.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mls_vec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf_16' is not defined"
     ]
    }
   ],
   "source": [
    "with open('linear_svmclf_feat_vec.txt', 'w') as ls_vec:\n",
    "    pickle.dump(clf_16, ls_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
